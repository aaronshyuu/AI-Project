{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taxi Q-Learning Project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By: Aaron Shyuu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem & Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I will use the new taxi-v3 environment in OpenAI Gym to solve a Reinforcement Learning problem and test my Reinforcement Learning algorithms. Furthermore, I will use visualizations to compare the two RL algorithms.\n",
    "\n",
    "### Goal: \n",
    "\n",
    "In this game, the taxi is our agent and it needs to deliver a passenger to their destination.\n",
    "\n",
    "### Description: \n",
    "\n",
    "There are four designated locations in the grid world indicated by R, G, Y, and B. When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger's location, picks up the passenger, drives to the passenger's destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends.\n",
    "\n",
    "### Specifications: \n",
    "\n",
    "Number of states -> 500 \n",
    "\n",
    "25 grid locations (of taxi) x 5 possible passenger locations (passenger can be in taxi) x 4 possible destinations = 500 \n",
    "\n",
    "Possible actions -> 6 discrete and deterministic actions\n",
    "\n",
    "Move South, North, East, West, Pickup passenger and Dropoff passenger\n",
    "\n",
    "Possible rewards -> \n",
    "\n",
    "Each action/time step gives reward -1, Successfully delivering passenger gives reward +20, Illegal move (pickups and dropoffs) costs -10\n",
    "\n",
    "### Display: \n",
    "\n",
    "R,G,B,Y are locations, blue indicates passenger, magneta indicates destination, yellow indicates empty taxi and green indicates full taxi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 486,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /opt/conda/lib/python3.7/site-packages (0.17.1)\n",
      "Requirement already satisfied: scipy in /opt/conda/lib/python3.7/site-packages (from gym) (1.3.1)\n",
      "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /opt/conda/lib/python3.7/site-packages (from gym) (1.2.2)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /opt/conda/lib/python3.7/site-packages (from gym) (1.5.0)\n",
      "Requirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from gym) (1.14.0)\n",
      "Requirement already satisfied: numpy>=1.10.4 in /opt/conda/lib/python3.7/site-packages (from gym) (1.18.1)\n",
      "Requirement already satisfied: future in /opt/conda/lib/python3.7/site-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.18.2)\n"
     ]
    }
   ],
   "source": [
    "# Dont't forget to install gym\n",
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[34;1mB\u001b[0m: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# A taxi environment \n",
    "env = gym.make(\"Taxi-v3\")\n",
    "env.reset()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Action Space Discrete(6)\n",
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "# Can verify state space\n",
    "print(\"Action Space\", env.action_space)\n",
    "print(\"State Space\", env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RL Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will be using Q-learning (an off policy RL algorithm) and another RL algorithm (likely an on policy algorithm like SARSA) to train my agent, determine the performence of each algorithm and compare them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Justification for Method"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I have decided to use an off policy and an on policy RL algorithm to compare the performences for this problem. Q-learning is off policy so policy is updated greedily and independent of next policy. However, an on policy RL algorithm like SARSA will continually improve the policy (ie. policy is updated to improve next policy) It will be interesting for comparison. I have considered deep learning (neural networks) which can be implemented but it is excessive since our state space is not huge.\n",
    "\n",
    "In terms of exploration and exploitation, SARSA variants explore while Q-learning does not. This means that SARSA is more conservative and allows penalties from exploratory moves. Q-learning attemps to find optimal solution directly while SARSA avoids dangerous paths. Clearly, there are pros and cons for each RL algorithm and it is worth analyzing.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "\n",
    "Q Learning Algorithm:\n",
    "\n",
    "Initialize Q-values Q(s,a) for all pairs\n",
    "\n",
    "Repeat until completion ->\n",
    "\n",
    "Choose an action a in current s based on current Q estimates\n",
    "\n",
    "Take that action and observe s' and r\n",
    "\n",
    "Perform update with Q(s,a) = Q(s,a) + alpha(r + gamma * maxQ(s'a') - Q(s,a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Q-Table\n",
    "\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Hyperparameters\n",
    "\n",
    "# Episodes for training \n",
    "episodesOfTraining = 50000\n",
    "\n",
    "# Maximum steps per episode\n",
    "max_steps = 99\n",
    "\n",
    "# Learning rate\n",
    "alpha = 0.1\n",
    "\n",
    "#Discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 0.2               \n",
    "max_epsilon = 1\n",
    "min_epsilon = 0.01         \n",
    "decay = 0.001       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_episode_rewards = []\n",
    "\n",
    "for episode in range(episodesOfTraining):\n",
    "    state = env.reset()    \n",
    "    done = False\n",
    "    reward_in_episode = 0\n",
    "    \n",
    "    # Exploration rate decay\n",
    "    if epsilon > 0.01:\n",
    "            epsilon = epsilon - decay\n",
    "            \n",
    "    for step in range(max_steps):\n",
    "        #Choosing an action given the states based on a random num\n",
    "        threshold = random.uniform(0, 1) \n",
    "         \n",
    "        # Explaration-exploitation tradeoff\n",
    "        #If threshold is larger than epsilon, exploit \n",
    "        if threshold > epsilon:\n",
    "            action = np.argmax(Q[state,:])      \n",
    "            \n",
    "        # Explore otherwise by choosing random action\n",
    "        else: \n",
    "            action = env.action_space.sample()\n",
    "            \n",
    "        # Perform action to get outcome state and reward\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Update Q-table\n",
    "        Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[new_state, :]) - Q[state, action]) \n",
    "        \n",
    "        # Update state and reward\n",
    "        state = new_state\n",
    "        reward_in_episode = reward_in_episode + reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "     \n",
    "    all_episode_rewards.append(reward_in_episode)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 493,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score over time: 5.06716\n",
      "*** Avg. reward per thousand episodes ***\n",
      "\n",
      "1000 : -95.74700000000021\n",
      "2000 : -6.782000000000019\n",
      "3000 : 6.867999999999965\n",
      "4000 : 7.329999999999952\n",
      "5000 : 7.4109999999999685\n",
      "6000 : 7.34799999999997\n",
      "7000 : 7.287999999999973\n",
      "8000 : 7.384999999999963\n",
      "9000 : 7.418999999999974\n",
      "10000 : 7.479999999999954\n",
      "11000 : 7.423999999999971\n",
      "12000 : 7.327999999999969\n",
      "13000 : 7.3689999999999625\n",
      "14000 : 7.31499999999996\n",
      "15000 : 7.454999999999958\n",
      "16000 : 7.2929999999999655\n",
      "17000 : 7.587999999999963\n",
      "18000 : 7.250999999999971\n",
      "19000 : 7.424999999999955\n",
      "20000 : 7.51899999999997\n",
      "21000 : 7.395999999999952\n",
      "22000 : 7.311999999999968\n",
      "23000 : 7.612999999999972\n",
      "24000 : 7.503999999999962\n",
      "25000 : 7.626999999999974\n",
      "26000 : 7.362999999999967\n",
      "27000 : 7.493999999999964\n",
      "28000 : 7.435999999999959\n",
      "29000 : 7.465999999999962\n",
      "30000 : 7.6319999999999615\n",
      "31000 : 7.344999999999964\n",
      "32000 : 7.315999999999951\n",
      "33000 : 7.46699999999997\n",
      "34000 : 7.42099999999996\n",
      "35000 : 7.613999999999962\n",
      "36000 : 7.49099999999996\n",
      "37000 : 7.4029999999999685\n",
      "38000 : 7.151999999999969\n",
      "39000 : 7.340999999999963\n",
      "40000 : 7.514999999999962\n",
      "41000 : 7.557999999999969\n",
      "42000 : 7.460999999999961\n",
      "43000 : 7.358999999999964\n",
      "44000 : 7.452999999999966\n",
      "45000 : 7.352999999999964\n",
      "46000 : 7.631999999999971\n",
      "47000 : 7.408999999999963\n",
      "48000 : 7.407999999999967\n",
      "49000 : 7.460999999999966\n",
      "50000 : 7.388999999999963\n"
     ]
    }
   ],
   "source": [
    "# Training score over time\n",
    "print (\"Training score over time: \" + str(sum(all_episode_rewards) / episodesOfTraining))\n",
    "\n",
    "# Average reward per thousand episodes\n",
    "print(\"*** Avg. reward per thousand episodes ***\\n\")\n",
    "reward_per_k_episodes = np.split(np.array(all_episode_rewards), episodesOfTraining / 1000)\n",
    "count = 1000\n",
    "for r in reward_per_k_episodes:\n",
    "    print(count, \": \" + str(sum(r/1000)))\n",
    "    count = count + 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd7gU5fnG8e/NoXekKL0JKoIiIAIqGitWjImJxhajQYymm0QlUWNixPQY04xJ1PwsMfYYiUpiLIkNG6CCAmJAEFGUooiU5/fHzB6Ww549u4ezp3F/rmuvM/NOe97dPfPszDvzjiICMzOzQjWp6wDMzKxhceIwM7OiOHGYmVlRnDjMzKwoThxmZlYUJw4zMyuKE4chqaWkkNSrrmMplqQnJJ1S13HUhIqfg6RbJH27lrZ9uaSf1/S8tUVSG0lrJHWr4fXOkHRiTa6zMWha1wFYbpLWZI22BtYBG9PxsyPixjzLTgCujoidSxii1RFJ3wW+no42TV8fpuNzI2JkseuMiCmlmLe2RMT7QNu6jmN74SOOeioi2mZewP+AY7LKKk0apSapiaRa/95IanA/ckoVc0RckvXd+Arw76zvxlZJoyG+d1a/OXE0UJJaSfqVpKWSFkv6kaRmkjoDdwID0kP3NZI6S9pX0pOSVkpaIulnhe5Q0tNBl0l6EvgA6CFpB0k3SHpT0iJJl2QSSlq2ezp8Vnr6ZUA6fp6kW9LhSmPKOm1zjqT5wOy0/ChJr0p6T9JPK8S5q6TH0vUtl3RDnjp9QtJL6XqmSxqUll8q6f8qzPs7ST9Mh/PVe7Kkf6Wfy7vABTm2W+3PoVCS2qbv3eT0vXshLb9G0huSVqUxjM5a5seSfpsOD5X0YfrZvSHpLUlfrea87dJTbu9JmiVpiqQ5eWLfQ9JDkt5NP59jsqbdJukXkh6WtFrSg5J6VKjzTun48ZLmpvMtknRu1nq+JGmBpLfTdXbLmnaspHlpvD/KEd8X0vWukHRv1vbLJP0m/d6tlPS8pEZ7xO/E0XB9F9gDGAaMBA4EvhkR7wAfBxZk/Qp9B1gPnAfsAOwPHAOcVcT2TgFOA9oBbwI3AiuBAcBo4Djg1HTeR9J4AMYDC4ADssYfTocLienotH57pTuFW0lO03QFlgOjsua9ArgL6Aj0AX6XqyKShgLXAV8AuqXx3JPuwG8CJkpqlc7bDPhkWk4V9c7U73mgC/CTHJvf1s+hGEcCI9j8Hj0GDAU6A/cBt+ZJWs1Jvl8DgGOBKyX1rca8V7D58ziW5DuUk6SOwAMkn1sX4HPA9ZL6Z812KvBNks//deCPOdajtPwzEdEO2Av4TzrtWODCNJbeJJ/l9em0nsDNwJfT9a8Ehmet9xTgXOAoYEdgVmZZku/BUGAg0CmNc2VldW3wIsKvev4CFgKHVCh7Azgoa3wiMCcdngDMq2KdFwA3p8MtgQB6VTLvE8BFWeN9gfeBZlllZwDT0uFzgVvT4QXA54Hr0vE3gSFFxDQua/okktMymfEy4C3glHT8VuBqoHsVdb8cuKHCepYDY9LxGcCn0uFjgJcKrPdk4JUiP9tKPwfgFuDbVSw/GZheoaxtup7ReZYrI2kXGZiO/xj4bTo8NF2+Y9b8LwFHV2Pet4B9s6Z9JfM9zRHTmZn3MqvsRuDr6fBtwLVZ07qm294hq847AQLeJklSbSus7y/AxVnjndPlupD8kJieNa0p8A5wYjr+KPDprOktSNodO5MkolnA3oC25f+9Ibx8xNEApb+odiL5xZXxOtAzzzJDJE2TtEzSKuBikn+WQi3KGu5LspNbnh7Svwf8guRXGCS/4A+Q1IdkR3sHMF7SriRHuS8XEVP2dntkj0fERpIEmvFVkgsJnpM0U5VfbdWDrPcuaz2Z9+8m4KR0+DMkO69C6l0x3q3UwOdQjC1iSU8TzZW0kmSH2DzPttdFxHtZ4x9QeeNzznnTo5muFeLI9/70BQ7KvLfp+zsR6J5r+YhYTpL8sqcTyV59IvBpYJGkf0oakU6u+Nm/Q/Id7cnW368NwJIK8V2bFdsy4COgF/A3kqOP3wNvSrpaUus8dW3QnDgaoPQf402SL3JGHzbvRHN1efx74FmSX5jtgctIfpkVvNms4UXAGqBTRHRMX+0jIvPP+SLJL9rJwMPpP+cakl+Aj6TxFxpT9naXkpxeAJKGerKSZUS8ERGfI9mRfAn4Y5q8KlpC1nsnqSxdT+b9+wtweHrq4hiS0xeF1LtivLls6+dQjPJYJB1B8nlMJDl11Jlkp1eqbWd2vG+T7FgzelcyOyTv731Z723HSE61np9reUldSRL50hzb/k9EZE4p/YvNyb/iZ78D0Ibks6/4/SojSSbZ8Z1SIb5WEfFCJH4cEcNJTm+NAr6Yp64NmhNHw3UzcImShu9uwBQg06i7DOgmKfsXYjtgZUSsUdJw/fnqbjgiXiM5ffXDtPGziaRBkvZLpwdJO8d5bG7PeLjCeHViugfYW9LRadvDN0hOUwAg6dOSeqTbz/wC3pBjPX8BPi5pfLqeC0h+gc9I438DeJKkHWRWRCwopN4FqrHPoUjtSN6Lt0mONC4HmtXCdm8Fvi2pvaR+wNl55r0dGC3pk5KaSmouaWyFRubjJe0tqQVJHR6MiBXZK0k/m09JakfSprSGzZey3wycLWn3tB3rSuD+iHgbuBsYK+mI9HvxLaBD1qp/C1wsaXC6nU6Sjk+Hx0oamR5lrSFJyhtppJw4Gq6LSc4lv0jSGPsf4IfptBdIdrKvp4fVO5CcxjlLyf0hvyLZeW6Lk0h+uc4BVqTryz5l8zDJzuqRSsYpNqaIWAqcCPycpE1iR9KdfWos8Ey6vr8CkyJiSY71zCQ5n/67dD0HAxPTX8gZNwGHsLlRvNB6V6WmP4dC3QU8DrxG0u70BvBuLWz3QpId6SKS0zl/IbknaStpAjicJJm+SXJ0cBlbJrg/k7SxvE3SEP25SrY7Kd3meyTfmTPSbdyVLn8vsJjkyOv0dNpi4GSSdrLlJD9Kns+K788kyeOu9DTj8yTfHdJ5b0i3twCYT/L5NkrafNbAzKy0JH0DODA9jVTssrcBMyJias1HZsXwEYeZlYykvpJGp6f1hpGc97+zruOybeM7Ss2slFqSXG3Uh+TU3g0kbUfWgPlUlZmZFcWnqszMrCiN/lRVly5dol+/fnUdhplZg/LMM8+8HRFdc01r9ImjX79+zJgxo+oZzcysnKTXK5vmU1VmZlYUJw4zMyuKE4eZmRXFicPMzIrixGFmZkVx4jAzs6I4cZiZWVEa/X0cNWXRig+44I6ZNC9rwsq16zlo124sWrGWnTq05JQxfdmhTXNO++OTvPHuWn58wp58/a8v0KysCV85ZBB3PfcGnx3XnzueXczyNes458CBjBvYhdfefp+l761l5dr1jO6/A2vXb+Ss62cw5ajdaNOiKW+8u5Zn//cuH9ulG21aNGVk307l8fzkgbns1acj7Vo241u3zeT6z43m77OW0r1DS47dswefv+EZxg7szL/nvkWLpmV8dlw/durQgl//ez5XfmIP5ixdzYMvL6Nf59bMeXM1zy96jzfeXUvPTq1Y8t5ajt+rJzt1aEXzpk34xT9f4ZMjejP95WVcdORufPDRBg7eLelJ/J4Xkl7L5yxdxVcPHcxhP3uEXp1a0aysCW1aNGXm4vfo27kNnx3Xl1/8cx4ffrSRaV/en98/uoCdOrQE4KnXVtC2ZVP2HdiFDz7awDWPLODEvfvQrKl4Z81H7NGrI2f86Sm+dcSunDa23xafy99eWMKyVR8ysm8nfvzAXF5b/j4/OH4YLyxayc+mv8Jd5+7LG++u5dK/vciYAZ355uG78PTCFXzjtplMHN6DxSvWMm/5Go7fqyed27bgz48vZHifjtw3602+N3F3Tt6nL7c9u5hxAzvz0Jy3uPRvLzFxeA/uePYNRvbtxGfH9WNozw4sfW8t43ZOHqb31xmL+P2jCzj3Yztz0K7d+OfLb3Hwbt0YdukDHD+iJ0J0aducxe+u5byDdubPT7zOgC5tGNitLWf86WkA/jJpDDNef5cDBnelZbMybp2xiJbNyhi8Y1v+t+IDzh4/kKnTXmbZqnVMGj+AdRs2Mf+tNRy8Wzcm/OJRlq9ex0PnH0j/Lm3YuCm48cnXmfPmapat/JCTx/Th3ffXc+Sw7rRqXsbnrnuaf815i/u+tD9DerTn2kcXcPVD87jm1FGM7p887mTCzx9hzpuruee8fVmzbgPPL3qPPju0Zt5bazh2zx7MfXM1RwxLHsT31qoPuejO2Ry1x05c88hrHL9XT1auXc/Ivp0oayL6dm7NlDtn86m9e/Olm5/jga+O54VF7/HJkb2Y/cYqnnztHf634gOG9ujAJ0f24oP1Gznr+qe5+OjdeeZ/7/LXGYsY0r093zh8F868fgY7d2vLpggOG7Ij1//3dcYP7sq+O3fm9mcW8+4H6zl0yI58tGETXdq14L6ZSzllTF+G9Uoes7Fuw0bO+NPTXHv6KD7asIk/PPYaB+7SlZF9d+DD9Rv5/A0z+Pmnh3PBHbOYevwwXl66mrfXrGPnbm0Z2rMD6zZs5OK7XuT+l97ksW8dRNsWyS51+kvL2KFtc67/70KOHNadu557g2mz3+Sk0b05ZLcdWb56HRsjePSVt7n4mCH06NiK6S8tY8UHH9G2RVMi4Lybn+XJCw/mT/9dyJML3qFT6+Z0aduCOW+u4v2PNrLkvbXMvOQwfvzAK0QE985cytkHDGDfnbtw4e2zOGVsX47dswel0Oj7qho1alRs6w2A76/bwO6X3F9DEZmZ1Y75PziSsibVe8ijpGciYlSuaT5VVYCL736xrkMwM6s3nDiqcM8LS7j92cV1HYaZWdFK9UB5J44qfOnm5+o6BDOzesWJw8yskfpo46aSrLfBJQ5JEyTNlTRP0gWl3NZLS1aVcvVmZiWlEp2ralCJQ1IZ8CvgCGAIcJKkIaXa3pFXPVqqVZuZlVyLpmUlWW+DShzAaGBeRCyIiI+AW4CJdRyTmdl2paEljp7AoqzxxWmZmZnVkoaWOHKdsdvqDkZJkyTNkDRj+fLltRCWmdn2o6EljsVA76zxXsCSijNFxDURMSoiRnXtmvORuWZmVk0NLXE8DQyS1F9Sc+BE4J46jqleOWl0n7oOoeRuP2dsXYewld+eMmKrssE7ti35dn/4iT22KrvxrH3Kh/cf1IX90j60tgctm+XfpfXq1KqWIinOJcfU/DU+3zh8lxpfZ0aDShwRsQE4D7gfeBm4NSIaXH8g3zh8Fz62S9VHQpl/+Nsmj+UXJw7nq4cMzjnfL0/ai9vPGctvTh7BFccP47UrjuSiI3flqYsOLjim7x03lFmXHsbXDx3Mrz4zgtvPGcsfTt/cTc3t54zj0W9+rLzfmznfm1Dpun58wp5ce1qy7MCubVg49SjOP2wwJ4zsxaxLD+Nrhw5m7ve3XP6K44flXFf2P/qpY/qycOpR9OzYusr6LJx6FDMvPazK+bK38+g3P0bnNs3Ly67/3GjO2LdfQctPGNqdhVOPYuHUo2jTPLmSZVS/pIPArx06mLvO3ZfPjkvWNaJPx/J5f/bpPfn2Ubttsa7bzxlbPn3h1KMqrd/CqUfxqb17s3DqUey7c2d++qk90+Eu/PrkEdx+zjj+fOY+/N9Z+/DXyZuT7e9P27L7oUuOGcLCqUdx7xf3Ky/bs3dHZn/3cO794n6MHdB5q+3/+czRfG/i7gCcfcAATh/bd4vpk8YP4ISRvZL3Zvedtlr+K4cM4tKsneUz3z6EYT2TjgeP36vnFvNV1LF1s5zvCcAtkzbX87KJu9Nnhy2/K6eOSeL88Ql7ln8eVxw/jIVTj8r7nf7dqSO3Kpty5G6cMmbLH2onjOzF9Z8bvUXZ5/fvzx8/O4qbztqHxy88qHy72c7Yt/8W4/d+cT/6d2nDx3bpytWf2Ys/fXZvFk49iqs/sxffnJAkhOvO2JvXrjiSsw8YkDPmM/frn7O8JriTwzz6XfD3opeRYK/eHXn2f+/lnP6Zffrwg48PY8PGTcxdtpo/P/467Vo2ZeLwntzy9P/44kGD2LF9S1auXU+HVlv/g2RievXyI/hg3UaaNRWtm1feyXF2HY7aoztHDN2JQ3bbkV2/8w96dWrF+MFd+fqhg+nctkXO5d9Zs47WzZvSKt0ZbtwUfPDRBtq1bMaadRu44PaZ3DtzKV3aNuftNR8BlO/s1n60kbImonnT3L9PMrG9dNnhtG7elA/XbwRgw6bgmkcWsHe/TuzTvzMbN0X59jNWfbieZk2a8ODLy7a6uz/Ty2vF+n92XD+u++9CvnTwIA7dbUcefOlNvnjwINZv3LTFe5hZJlOPlR+sZ8/LHgCSHcN3jhlCy6ZlrP5wPSO/P53durdn2pf3L18+U4/v3fsSNz75P7533FBOHdOXRSs+YP8fPsTZBwzgwiO2TBbrN25i0JRpjBvYmZs+Pybn+wTwkxP25Og9u1frMstxV/yTJSs/ZNqX92dg17bs+p1pbAqY/d3Dy3t1PeNPT/HQ3OX84fRR5T0gA2zYuImdp0wD4IVLDqNDq2ZEBKvXbaB9y+R7unLtelqkn3XLZkl8qz9cT5vmTRlw0X0APPedQ2nWtEn59rLf6+/f+xLXPvYap4/ty/WPv07Pjq34zwUHlc/z65NHMG5gZ9q3bMbSVR/StW0LHnllOQvfeZ+u7ZLvb+8dWnP8r//L8N4duevcfXl+0Xsc96v/lNfjujP2ZkTfTlvE3L5lU5Te8LD6w/W0bt6Unz34Clc/NI+u7VqwfPU6rjl1JPsN6kKzsiasWrt+i/+JC26fyS1PL+ILBw7k64ftwiOvLOeM655m7IDO/OmMvcvfi2xr1m1gw8ZNtGpeRkTyfmXqWUjHhKs+XF9eB0h6+Y2AdRs2bfUZVFe+Tg7drXoNu+6M0Ux/aVmliSPzdWha1oTde3Rgataphu/33PyrO1fSyNasrAkdWhd3wPiJET05aNdkZ/D4hQfRrmWz8n/gylRMKGVNRLv0C9u2RdPyL+fO3dry9poV7JN2ww1stbOvTGannf1F/9qhuY+uMjL/NMfu2WOLxPGvrx/AgK65TxG1aZGsv1kTMaxXh/KutZuVbfk+nrh3b0b02dyFfYesX7jS5m13btuCxy88aIt/4Ox6TD5gILPeWMnRaXfjvXdozUPnH0jvHKdLmpU14bFvfYzObXIncICRfTtxxLCdqn1t/j4DOnPnc2/QrmVTmjdtQoumZaxdv5HsfZQquWOsadZ7lPluStqi7rm+s+0qvDedso7oKrMp/S3bpMLXu02LpnRsnSzfs2PyHh4yZMct5nnm9RXJsmk1Kv4wblJFzJl4v3roYI4f0ZMf3DeH6S8vAzZ/Tyv+T2Tq1KZFU8qaiE3pNls1L6t0553v/66Q3mwrfucy34ltTRaFcuKoYU0E3z56N47eozv/mvsWv3t4ARcduStrP9rEz6a/stVOqrZIMLz35p1h9w41e65XaUrs0bH2zyHf+8X9OPqXjwFUmjR2696ezD6kqrtpp+ZoN8hQhQv78r2PvXdozT3n7bdFWf8ubSqdv1en/Kfgbj9nXN7pVbni+GF8Zp8+5dvJ7OCaFHh78TF79thqR1wKO7ZvQZvmZVsdle2RnsrKJ9PDRqZOO3drS/OyJuVdbxR6J3VZE1X6Xaqo4ltSnvhK1cNgPeDEUcOaSLRoWsY+Azqzz4DO5V/+ax9dAJSuC4CqvHZF7nPl22ri8B7c9sxiJh0wgMcXvMPE4YU/OGbS+AFc88iCbY5haM8O/GXSmPLTFdmu/9xoBnVrS4+OrbjyH3OAyn9VF+KYEj0YJ5/+XdpUeQRaiJbNyti73+Yjwo6tm7Fs1botvpP5EsMvT9prm2Oo6MGvjmfVh+sBmDB0J6597DUO2nVHzjtoc9vGXefuS/OyJgUdrTQtSyqTeb/atWzGK5cfweQ/P8M/XnyT5iX84ZZ5H5ulMVQ8KijE5/cvXbtETXLiqGGV/XrbKz31URdXuByVniophf0HdS1vC6isIbcyFx25GxcduVvVMxZgnxwNuAAHDN58EcKYAZ35zb/nMyrrSYrF2m9Q7X9+D51/YEnW+9ezx/HovOVbnPo6dMhOPDR3OQML/LVdqD+fOTrn/8agHduVD4/qt0PO79Dw3h0L3s5evTty0ZG78smRvbcon/qJYYzo27H8iYaFOnRIN6a/vIydu1X+fmSOLDL1Gz+oK9+asCsnjynuCsdi/3/qkhvHKxER9L/wvqKX+/f5B9KvktMR76/bQJsq2hSqctszi2nXsimH57hSJZcP12+kWVmTaj8FrLGp7mfw3/lv8+qyNZye44qYxiQi+OCjjdv8PW0sCnk/Vn+4niv/MYcpRw4puF2vIXDjeC2qLGkANfLP+Mn0EsdC1VZjWUNR3c9g3MAujBvY+O+HkOSkkaWQ96Ndy2Z8/7jcl5M3Vg3qPo76bpesw24zs8bKiaMGjRlQ3PlTM7OGyImjEo286cfMrNqcOCrhvGFmlpsTRw0amOeSPTOzxsKJowZlOlAzM2vMnDgqUZ37W7bljmQzs4bCicPMzIrixFEJN46bmeXmxFGJXz80v65DMDOrl5w4KnH3C28UNf+APF2NmJk1Jk4cNWT84KofBWtm1hg4cZiZWVHqXeKQdKmkNyQ9n76OzJp2oaR5kuZKOrykgbh13Mwsp/raf/LPIuLH2QWShgAnArsDPYDpkgZHxMZSBOC8YWaWW7074shjInBLRKyLiNeAecDoOo7JzGy7U18Tx3mSZkr6o6TMcz57Aouy5lmclm1F0iRJMyTNWL58ebUCKPbO8cb+JEUzs4w6SRySpkuaneM1EfgNMBAYDiwFfpJZLMeqcu6tI+KaiBgVEaO6dq3e1U6bnAfMzHKqkzaOiDikkPkk/R64Nx1dDGQ/gb4XsKSGQysXRbZyuJ8qM9te1LtTVZK6Z41+HJidDt8DnCiphaT+wCDgqdqOrzI+VWVm24v6eFXVDyUNJzkNtRA4GyAiXpR0K/ASsAE4t1RXVJmZWeXqXeKIiFPzTLscuLw24li0Ym1R83dq07xEkZiZ1S/17lRVQ/WFA3eu6xDMzGqFE0cNad7Ub6WZbR+8tzMzs6I4cZiZWVGcOMzMrChOHGZmVpRKL8fN7s48l4i4r+bDMTOz+i7ffRyZ+yl2AMYBD5P0F7V/OuzEYWa2Hao0cUTESQCS7gaGRMSidLw38NPaCc/MzOqbQto4BmaSRmoxsGuJ4jEzs3qukC5H/iPpHuAmkv6jTgL+U9KozMys3iokcZwDfBo4IB3/S/oyM7PtUN7EIakM+FtEHAncXDshmZlZfZa3jSPttnyTpHa1FI+ZmdVzhZyqWgk8L+kfwPuZwoj4ZsmiMjOzequQxPFI+jIzM6s6cUTE72ojEDMzaxiqTBzp872/BwwBWmbKI2JICeMyM7N6qpBTVdcDV6avjwNnAOtLGVRDcsqYPhw6ZKe6DsPMrNYUcud4m4j4OxARMTciLgAO3JaNSjpB0ouSNkkaVWHahZLmSZor6fCs8pGSZqXTrpKkbYmhpnz/uGEcMLhrXYdhZlZrCkkc69Kd9AJJZ0k6AthxG7c7GzieCo3ukoYAJwK7AxOAX6f3kgD8BpgEDEpfE7YxBjMzq4ZCEsf5QFvgy8DBwHnAmduy0Yh4OSLm5pg0EbglItZFxGvAPGC0pO5A+4h4PCICuAE4bltiMDOz6imkjWNhRKwGVpP0U1VKPYEnssYXp2Xr0+GK5TlJmkRydEKfPn1qPkozs+1YIYnjTkkdgcdJTi09GhGvVrWQpOlArlbjKRFxd2WL5SiLPOU5RcQ1wDUAo0aNqnQ+MzMrXiH3cewjqTUwluQhTv+U1Cwiulex3CHViGcx0DtrvBewJC3vlaPczMxqWSH3cYwiSRjjge7AdODREsVzD3CTpJ8CPUgawZ+KiI2SVksaAzwJnAb8skQxmJlZHoWcqnoKeBr4AXBv2vHhNpH0cZIdf1fg75Kej4jDI+JFSbcCLwEbgHOztncOcB3QCpiWvszMrJYVkji6A/sBBwFfl/Q+8FhEXF7djUbEncCdlUy7HNhq3RExAxha3W2amVnNqPJy3IhYBjwLPAe8CuwGHFviuMzMrJ4qpI3jVWAR8BhwC/CliHg//1JmZtZYFXKqakhEuG8qMzMDCrtzvLekv0t6BkDSMEl+iJOZ2XaqkMRxLUnPuJl5Z5NcDmtmZtuhQhJHu4go74ww7StqQ+lCMjOz+qyQxLFCUh/SLj4kHQMsK2lUZmZWbxXSOH4eSW+0u0iaD7wLfKqkUZmZWb2VN3FIakJyVdWBkjoDioi3ayc0MzOrj/KeqoqITcA30uF3nDS2NHZA57oOwcys1hXSxjFN0nmSukpqnXmVPLIGYLwfGWtm26FC2zgAprD52RhB0nvtdi0qfySImVmjVcjzOPI+d8PMzLYvhZyqMjMzK+fEYWZmRXHiMDOzolTaxiFpSL4FI+Klmg/HzMzqu3yN43/IMy2AcTUci5mZNQCVJo6IGFuqjUo6AbiU5GmCo9PHwiKpH/AyMDed9YmImJxOG8nmZ47fB3w57XDRzMxqUSH3cSBpZ2AI0DJTFhG3bsN2ZwPHA7/LMW1+RAzPUf4bYBLwBEnimABM24YYzMysGgp5dOwFwDHAYGA6cCjJY2SrnTgi4uV03QXNL6k70D4iHk/HbwCOw4nDzKzWFXJV1cnAeGBJRJwE7EFy93ip9Jf0nKSHJe2flvUEFmfNszgtq1Mq6dtgZlY/FXKqam1EbJS0UVLbiFgiaWBVC0maDuyUY9KUiLi7ksWWAn0i4p20TeMuSbuTO1FV2r4haRLJaS369OlTVahmZlaEQhLHTEkdSRqmn5K0CnihqoUi4pBig4mIdcC6dPiZ9Pkfg0mOMHplzdoLWJJnPdcA1wCMGjXKDehmZjWokL6qzkoHr5J0P9AhIp4qRTCSugIr0iOcAcAgYEFErJC0WtIY4EmSZ57/shQxmJlZflW2cUgqb4COiLkR8VR2WXVI+rikxcBY4O9pQoKkLWWmpBeA24DJEbEinXYOcC0wD5iPG8bNzOpEvpnbey8AABQUSURBVDvHmwLNge6SWrG5naE90G9bNhoRdwJ35ii/Hbi9kmVmAEO3ZbtmZrbt8p2q+jJwPtAZWJBVvorc91+Ymdl2IN+d4z8BfiLp/Ij4cS3GZGZm9VghV1X9PL28dXw6/m/gTxGxsWRRmZlZvVVI4vgl0Am4IR0/BRhJ0lhtZmbbmUISx7iI2DNr/L70qiczM9sOFdLlyCZJ5bdfS+oNbCpdSGZmVp8VcsRxIfAfSS+SXJK7K2l3HmZmtv3Jdx/H3hHxdET8Q9KuQKbPqNkR8X6tRWhmZvVKviOO3wEjANJEUZJuRszMrGEppI3DzMysXL4jjgGS7qhsYkQcX4J4zMysnsuXOJYDv6qtQMzMrGHIlzhWR8Q/ay0SMzNrEPK1cSyqtSjMzKzBqDRxRMTE2gzEzMwaBl9VZWZmRXHi2AZHDN2prkMwM6t1VXY5ImmPHMUrgUURsV33WdWvS5u6DsHMrNYV0lfVH4DhQKavqt2A2UAHSZN85ZWZ2falkFNVrwIjI2J42r36SOB54HDgJ9XZqKQfSZojaaakOyV1zJp2oaR5kuZKOjyrfKSkWem0qyQp99rNzKyUCkkcu0XEzMxIRMwCRkTEvG3Y7oPA0IjYA3iFpAdeJA0BTiTpUHEC8GtJZekyvyHplXdQ+pqwDds3M7NqKiRxzJf0S0n7pq+rgHmSWgAbqrPRiHggIjLLPgH0SocnArdExLqIeA2YB4yW1B1oHxGPR0SQPI3wuOpse1v1d7uGmW3nCkkcpwGLgQtIjgyWAKeTJI2DayCGzwHT0uGebHnj4eK0rGc6XLE8J0mTJM2QNGP58uU1EOJmD51/YI2uz8ysoamycTwiPgCuTF8VraxsOUnTgVzXq06JiLvTeaaQJKAbM4vlCiFPeWUxXwNcAzBq1KhK5zMzs+IVcjnuGOASoG/2/BExON9yEXFIFes9HTgaODg9/QTJkUTvrNl6kRzhLGbz6azscjMzq2WFXI77J+CbwDPAxprYqKQJwLeAA9Ijmox7gJsk/RToQdII/lREbJS0Ok1iT5KcPvtlTcRiZmbFKSRxrIqIv9Xwdq8GWgAPplfVPhERkyPiRUm3Ai+RnMI6NyIyyeoc4DqgFUmbyLSt1lpLPjGiF02b+GpgM9s+FZI4/iXpCuAOYF2mMPsS3WJFxM55pl0OXJ6jfAYwtLrbrEk/+dSedR2CmVmdKSRx7FfhLyQN0+NrPhwzM6vvCrmqav/aCMTMzBqGShOHpJMi4mZJX8o1PSKuKl1YZmZWX+U74uiU/u1aG4GYmVnDUGniiIhfp3+/U3vhmJlZfVfIDYBdSLoF6ceWNwBOKl1YZmZWXxVyVdXdJB0RPkYN3QBoZmYNVyGJo01EfL3kkZiZWYNQSO+40yQdVvJIzMysQSgkcUwG/iFpjaQVkt6VtKLUgdVHC6ceVdchmJnVuUJOVXUpeRRmZtZg5LsBcFBEvEryGNdcqt1XlZmZNVz5jjguAM4EfpVjmvuqMjPbTuW7AfDM9K/7qjIzs3KFtHEgaVdgCNAyUxYRN5UqKDMzq78KuXP828BhwK7A/cDhJDcDOnGYmW2HCrkc99PAx4ClEXEqsCcFHqmYmVnjU0jiWJs+vnWDpHbAm8CA0oZlZmb1VSFHDs9J6gj8EZgBrAKeLWlUZmZWb+U94pAk4NKIeC8ifgUcBZwdEadty0Yl/UjSHEkzJd2ZJiYk9ZO0VtLz6eu3WcuMlDRL0jxJV6WxmZlZLcubOCIigHuzxudFRE0cbTwIDI2IPYBXgAuzps2PiOHpa3JW+W+AScCg9DWhBuIwM7MiFdLG8ZSkETW50Yh4ICI2pKNPAL3yzS+pO9A+Ih5Pk9kNwHE1GZOZmRWm0sQhKdP+sR9J8pgr6VlJz0mqyTaOzwHTssb7p9t4WFLm5sOewOKseRanZZXFPknSDEkzli9fXoOhmplZvsbxp4ARVPOXvaTpwE45Jk2JiLvTeaYAG4Ab02lLgT4R8Y6kkcBdknYHcrVnRGXbjohrgGsARo0aVel8ZmZWvHyJQwARMb86K46IQ/JNl3Q6cDRwcHr6iYhYB6xLh5+RNB8YTHKEkX06qxewpDpxmZnZtsmXOLpK+lplEyPip9XdqKQJwLeAAyLig6zyrsCKiNgoaQBJI/iCiFghabWkMcCTwGnAL6u7fTMzq758iaMMaEvu00Tb6mqgBfBgelXtE+kVVOOByyRtIHm++eSIyDw06hzgOqAVSZvItIorNTOz0suXOJZGxGWl2GhE7FxJ+e3A7ZVMmwEMLUU8ZmZWuHyX4/oGOzMz20q+xHFwrUVhZmYNRqWJI6ttwczMrFwhd46bmZmVc+IwM7OiOHGYmVlRnDjMzKwoThxmZlYUJw4zMyuKE4eZmRXFicPMzIrixGFmZkVx4jAzs6I4cZiZWVGcOMzMrChOHGZmVhQnDjMzK4oTh5mZFcWJw8zMilIniUPS9yTNlPS8pAck9ciadqGkeZLmSjo8q3ykpFnptKsk+dG2ZmZ1oK6OOH4UEXtExHDgXuBiAElDgBOB3YEJwK8llaXL/AaYBAxKXxNqI9CRfTvVxmbMzBqMOkkcEbEqa7QNEOnwROCWiFgXEa8B84DRkroD7SPi8YgI4AbguNqItWfHVrWxGTOzBqNpXW1Y0uXAacBK4GNpcU/giazZFqdl69PhiuWVrXsSydEJffr0qbmgzcysdEcckqZLmp3jNREgIqZERG/gRuC8zGI5VhV5ynOKiGsiYlREjOrateu2VsXMzLKU7IgjIg4pcNabgL8Dl5AcSfTOmtYLWJKW98pRbmZmtayurqoalDV6LDAnHb4HOFFSC0n9SRrBn4qIpcBqSWPSq6lOA+6u1aDNzAyouzaOqZJ2ATYBrwOTASLiRUm3Ai8BG4BzI2Jjusw5wHVAK2Ba+jIzs1pWJ4kjIj6RZ9rlwOU5ymcAQ0sZl5mZVc13jpuZWVGcOKrQxPenm5ltwYmjCpccs3tdh2BmVq84cVShU5vmdR2CmVm94sRhZmZFceIwM7OiOHGYmVlR6qyTw4bkujP25q3V6+o6DDOzesGJowAH7tKtrkMwM6s3fKrKzMyK4sRhZmZFceIwM7OiOHGYmVlRnDjMzKwoThxmZlYUJw4zMyuKE4eZmRXFNwBW4o+fHcVHG6KuwzAzq3fqJHFI+h4wkeSZ428Bn42IJZL6AS8Dc9NZn4iIyekyI9n8zPH7gC9HRMn27AftumOpVm1m1qDV1amqH0XEHhExHLgXuDhr2vyIGJ6+JmeV/waYBAxKXxNqL1wzM8uok8QREauyRtsAeY8cJHUH2kfE4+lRxg3AcSUM0czMKlFnjeOSLpe0CDiZLY84+kt6TtLDkvZPy3oCi7PmWZyWVbbuSZJmSJqxfPnyGo/dzGx7VrLEIWm6pNk5XhMBImJKRPQGbgTOSxdbCvSJiL2ArwE3SWoPKMcmKj1KiYhrImJURIzq2rVrzVbMzGw7V7LG8Yg4pMBZbwL+DlwSEeuAdenyz0iaDwwmOcLolbVML2BJDYZrZmYFqpNTVZIGZY0eC8xJy7tKKkuHB5A0gi+IiKXAakljJAk4Dbi7lsM2MzPq7j6OqZJ2Ibkc93Ugc/XUeOAySRuAjcDkiFiRTjuHzZfjTktfZmZWy+okcUTEJyopvx24vZJpM4ChpYzLzMyqphLeQ1cvSFpOclRTHV2At2swnIbAdd4+bG913t7qC9te574RkfPqokafOLaFpBkRMaqu46hNrvP2YXur8/ZWXyhtnd3JoZmZFcWJw8zMiuLEkd81dR1AHXCdtw/bW523t/pCCevsNg4zMyuKjzjMzKwoThxmZlYUJ44cJE2QNFfSPEkX1HU8xZL0R0lvSZqdVbaDpAclvZr+7ZQ17cK0rnMlHZ5VPlLSrHTaVWl3L0hqIekvafmT6QO46oyk3pIekvSypBclfTktb8x1binpKUkvpHX+blreaOucIaks7UH73nS8UddZ0sI01uclzUjL6rbOEeFX1gsoA+YDA4DmwAvAkLqOq8g6jAdGALOzyn4IXJAOXwBcmQ4PSevYAuif1r0snfYUMJakd+JpwBFp+ReA36bDJwJ/qeP6dgdGpMPtgFfSejXmOgtomw43A54ExjTmOmfV/WsknaPe29i/22kcC4EuFcrqtM51/iWob6/0jb0/a/xC4MK6jqsa9ejHloljLtA9He4OzM1VP+D+9D3oDszJKj8J+F32POlwU5K7U1XXdc6K9W7g0O2lzkBr4Flgn8ZeZ5Kesf8JHMTmxNHY67yQrRNHndbZp6q21hNYlDWe96FRDciOkfQyTPq3W1peWX3zPTyrfJmI2ACsBDqXLPIipIfZe5H8Am/UdU5P2TwPvAU8GBGNvs7Az4FvknSQmtHY6xzAA5KekTQpLavTOtdV77j1WVEPjWoEKqtvvvehXr5HktqSdJL5lYhYlZ7CzTlrjrIGV+eI2AgMl9QRuFNSvk5AG3ydJR0NvBXJs3oOLGSRHGUNqs6pfSNiiaRuwIOS5uSZt1bq7COOrS0GemeNN5aHRi1T8uz2zDPc30rLK6tvvodnlS8jqSnQAVhBHZLUjCRp3BgRd6TFjbrOGRHxHvBvYAKNu877AsdKWgjcAhwk6f9o3HUmIpakf98C7gRGU8d1duLY2tPAIEn9JTUnaSy6p45jqgn3AKenw6ez+UFY9wAnpldW9Cd5eNZTkf/hWdnr+iTwr0hPkNaFNL4/AC9HxE+zJjXmOndNjzSQ1Ao4hOSBaI22zhFxYUT0ioh+JP+X/4qIU2jEdZbURlK7zDBwGDCbuq5zXTb61NcXcCTJlTnzgSl1HU814r+Z5Pnt60l+TZxJcs7yn8Cr6d8dsuafktZ1LumVFmn5qPRLOh+4ms09DbQE/grMI7lSY0Ad13c/kkPrmcDz6evIRl7nPYDn0jrPBi5OyxttnSvU/0A2N4432jqTXN35Qvp6MbM/qus6u8sRMzMrik9VmZlZUZw4zMysKE4cZmZWFCcOMzMrihOHmZkVxYnDSkpSSPpJ1vj5ki6toXVfJ+mTNbGuKrZzgpKedx/KKhuW9lb6vKQVkl5Lh6cXue77M9fp55nnckkfq278Fda1OKun1ecl/ayK+T8u6Rs1sN3HJA3f1vVY/eAuR6zU1gHHS7oiIt6u62AyJJVF0mVHIc4EvhAR5YkjImYBw9N1XUdyT8FtObbTNJL+f3KKiMMrm5Y1z5QC4yzU/pHcbV6liLizhrdtjYCPOKzUNpA8+/irFSdUPGKQtCb9e6CkhyXdKukVSVMlnazk+ROzJA3MWs0hkh5N5zs6Xb5M0o8kPS1ppqSzs9b7kKSbgFk54jkpXf9sSVemZReT3GD4W0k/KqTCkg6RNF3SLSQ36SHpb0o6qXtR0llZ8y6W1FHSzul2/5DOM01Sy3Se/5N0XNb8lyp5HsVMSYPT8m6S/inpWUm/lvRG5s7yAmN+TNLPJT2evgej0vKzJP08HT4xjfGFzNGXpFaSrk+XeVbS+LS8taS/pjHeQnKTWWZbR6TbeVbJcyDapOU/kvRSusyVhcZutc9HHFYbfgXMlPTDIpbZE9iNpM+cBcC1ETFayUOavgh8JZ2vH3AAMBB4SNLOJN0prIyIvSW1AP4j6YF0/tHA0Ih4LXtjknoAVwIjgXdJeiM9LiIuk3QQcH5EzCgi/jEkz3H5Xzp+ekSskNQamCHp9oh4t8IyuwAnRcQsSXcAx5H0yVTRsojYS9KXSJ5NMRm4DPhHRPwoTaDn5IntUUmZo60/RsRV6XCLiBib1vda0iOqLJcAB0bEsqyk9CXgo4gYJml34D5Jg4DzgHcjYg9JewGZBxB1I3l+xMER8YGkKcCXJf2B5G7/3SMiikl6Vvt8xGElFxGrgBtIdjKFejoilkbEOpIuEjI7/lkkySLj1ojYFBGvkiSYXUn68zlNSZfjT5J0zzAonf+pikkjtTfw74hYnp5aupHkgVjV9XhW0gD4qqQXgMdJOpgbmGOZeekpMIBn2LKe2e7IMc9+pEkmIu4FVueJbf+IGJ6+rsoqvzld/l9ANyW9DWf7D3BDesSU2XfsB/w5Xe5Fko7zdiZ57/4vLX+OpLsMgHEkDxv6b/r5nJzWYQVJV+m/l/Rx4P088Vsd8xGH1Zafkzxs6E9ZZRtId0CSRPLExYx1WcObssY3seX3tmKfOZkupL8YEfdnT1DSFXdlO6RK+2CvpvLtSDqEZEc6JiLWSnqMrFM3WbLrvJHK/z/X5ZinJuLP9V5m+zzJw6KOBl6QtEcV283Vn5FIjoxO3WpCcnrsUJIODM8h+QFg9ZCPOKxWRMQK4FaShuaMhSSnhgAmkjwCtVgnSGqStnsMIOnY7X7gHCVdrSNpcOY8eh5PAgdI6iKpjOQJaQ9XI55cOgAr0qSxO8nRTU17DPgUgKQjSR6hW6xPp8sfSHI6rGKSHRARTwDfITmd1xN4hOSoAUm7kTxpbl6F8j2B3dN1/JfkfR6QTmsjaZCSK8vap0dLXyV5GJfVUz7isNr0E5Jz3xm/B+6W9BRJD5/VOT0xl2QHvyMwOSI+lHQtyemPZ9MjmeUk7QWVioilki4EHiL5VXxfRNydb5ki/B2YlJ6qmkOSpGraJcBNkk4G/gUso/L3M7uN47mIOCMdXiXpvyRJ54wcy/1MSVfdAh6IiNmS5gO/kzSLpDfm0yLiI0lXA9dLmklypDkDIG0fORP4i5LHFgBcBKwF7kjbpJqQtN1YPeXecc0agfQKrA0RsUHSfsDPI2JUEcs/BpwXEc+XLEhrNHzEYdY49ANuTk+zrQPOrttwrDHzEYeZmRXFjeNmZlYUJw4zMyuKE4eZmRXFicPMzIrixGFmZkX5f15aLt22CwVgAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Results and total reward over all training episodes\n",
    "\n",
    "plt.title('Total rewards over all Training episodes')\n",
    "plt.xlabel('Number of Training Episodes')\n",
    "plt.ylabel('Training Total reward')\n",
    "\n",
    "plt.plot(range(episodesOfTraining) , all_episode_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning is off policy and uses greedy approach to learn the Q-value. On the other hand, SARSA is on policy and uses action performed by current policy to learn the Q-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = np.zeros((env.observation_space.n, env.action_space.n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set Hyperparameters\n",
    "\n",
    "# Episodes for training\n",
    "episodesOfTraining = 50000\n",
    "\n",
    "# Maximum steps per episode\n",
    "max_steps = 99\n",
    "\n",
    "# Learning rate\n",
    "alpha = 0.1\n",
    "\n",
    "#Discount factor\n",
    "gamma = 0.9\n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 0.2               \n",
    "max_epsilon = 1\n",
    "min_epsilon = 0.01         \n",
    "decay = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_episode_rewards = []\n",
    "\n",
    "for episode in range(episodesOfTraining):\n",
    "    state = env.reset()    \n",
    "    done = False\n",
    "    reward_in_episode = 0\n",
    "    \n",
    "    # Exploration rate decay\n",
    "    if epsilon > 0.01:\n",
    "        epsilon = epsilon - decay\n",
    "    \n",
    "    # Explaration-exploitation tradeoff\n",
    "    #If threshold is larger than epsilon, exploit \n",
    "    if random.uniform(0, 1) > epsilon:\n",
    "        action = np.argmax(Q[state,:]) \n",
    "        \n",
    "    # Explore otherwise by choosing random action\n",
    "    else: \n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "    for step in range(max_steps):\n",
    "        \n",
    "        # Perform action to get new state and reward\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        # Explaration-exploitation tradeoff\n",
    "        if random.uniform(0, 1) > epsilon:\n",
    "            new_action = np.argmax(Q[new_state,:])            \n",
    "        else: \n",
    "            new_action = env.action_space.sample()\n",
    "        \n",
    "        # Notice that we update Q-table differently now with s, a, r, s' and a'\n",
    "        Q[state, action] = Q[state, action] + alpha * (reward + gamma * Q[new_state, new_action] - Q[state, action]) \n",
    "        \n",
    "        # Update state\n",
    "        state = new_state\n",
    "        \n",
    "        # Notice we need to update action for SARSA\n",
    "        action = new_action\n",
    "        \n",
    "        # Update reward\n",
    "        reward_in_episode = reward_in_episode + reward\n",
    "        \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    all_episode_rewards.append(reward_in_episode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training score over time: 4.93652\n",
      "*** Avg. reward per thousand episodes ***\n",
      "\n",
      "1000 : -100.7920000000001\n",
      "2000 : -7.4970000000000585\n",
      "3000 : 6.1849999999999765\n",
      "4000 : 7.430999999999955\n",
      "5000 : 7.528999999999957\n",
      "6000 : 7.417999999999965\n",
      "7000 : 7.12599999999996\n",
      "8000 : 7.535999999999965\n",
      "9000 : 7.475999999999956\n",
      "10000 : 7.402999999999961\n",
      "11000 : 7.269999999999964\n",
      "12000 : 7.396999999999964\n",
      "13000 : 7.453999999999966\n",
      "14000 : 7.187999999999972\n",
      "15000 : 7.517999999999962\n",
      "16000 : 7.361999999999963\n",
      "17000 : 7.3909999999999645\n",
      "18000 : 7.506999999999969\n",
      "19000 : 7.294999999999968\n",
      "20000 : 7.37999999999997\n",
      "21000 : 7.28599999999997\n",
      "22000 : 7.557999999999953\n",
      "23000 : 7.699999999999948\n",
      "24000 : 7.169999999999953\n",
      "25000 : 7.656999999999959\n",
      "26000 : 7.357999999999973\n",
      "27000 : 7.539999999999959\n",
      "28000 : 7.410999999999962\n",
      "29000 : 7.320999999999961\n",
      "30000 : 7.5299999999999665\n",
      "31000 : 7.490999999999972\n",
      "32000 : 7.481999999999968\n",
      "33000 : 7.380999999999962\n",
      "34000 : 7.390999999999965\n",
      "35000 : 7.438999999999968\n",
      "36000 : 7.287999999999962\n",
      "37000 : 7.493999999999974\n",
      "38000 : 7.35699999999997\n",
      "39000 : 7.392999999999969\n",
      "40000 : 7.303999999999961\n",
      "41000 : 7.519999999999965\n",
      "42000 : 7.544999999999963\n",
      "43000 : 7.384999999999965\n",
      "44000 : 7.279999999999959\n",
      "45000 : 7.404999999999968\n",
      "46000 : 7.393999999999972\n",
      "47000 : 7.387999999999971\n",
      "48000 : 7.676999999999959\n",
      "49000 : 7.529999999999967\n",
      "50000 : 7.573999999999973\n"
     ]
    }
   ],
   "source": [
    "# Training score over time\n",
    "print (\"Training score over time: \" + str(sum(all_episode_rewards) / episodesOfTraining))\n",
    "\n",
    "# Average reward per thousand episodes\n",
    "print(\"*** Avg. reward per thousand episodes ***\\n\")\n",
    "reward_per_k_episodes = np.split(np.array(all_episode_rewards), episodesOfTraining / 1000)\n",
    "count = 1000\n",
    "for r in reward_per_k_episodes:\n",
    "    print(count, \": \" + str(sum(r/1000)))\n",
    "    count = count + 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualization Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Visualizing results and total reward over all episodes\n",
    "\n",
    "plt.title('Total rewards over all Training episodes')\n",
    "plt.xlabel('Number of Training Episodes')\n",
    "plt.ylabel('Training Total reward')\n",
    "\n",
    "plt.plot(range(episodesOfTraining) , all_episode_rewards)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion & Discussion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After implementing the Q-learning and SARSA algorithms to solve this taxi problem, Q-learning does win by a tiny margin in terms of training score over time. For example, with episodesOfTraining set to 50000, Q-learning has a training score over time of 5.06716 versus 4.93652 of SARSA. (which are decent scores) With the same hyperparameters set, q-learning is consistently slightly outperforming SARSA. This means that for this taxi problem and its environment, the off policy Q-learning algorithm is the slightly better approach because it doesn't explore like SARSA. Q-learning exploits greedily as we can see in the implementation of my code. Specifically, it takes more risks to find an optimal solution wheras SARSA is more conservative and takes improved policies. I also printed the average reward per thousand episodes to make the comparison of the two algorithms even more apparent. It is interesting to note that by the time it got to near 50000 episodes of training, SARSA has a slightly higher average per thousand score. We can conclude that the difference between using the two algoithms for this taxi problem is quite negligible but Q-learning might have a slight edge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Troubleshooting\n",
    "\n",
    "The most difficult part of this project is the fine tuning of hyperparameters for each RL algorithm. I have spent many hours tuning the alpha (learning rate), gamma (discount factor) and epsilon (exploration variable). After narrowing down the values a lot and with episodesOfTraining set at 50000, I have tried alpha = 0.1, gamma = 0.9 and epsilon = 0.5 which yielded a training score over time of 4.81114 for Q-learning and 4.76118 for SARSA. When I continued to experiment with alpha = 0.1, gamma = 0.9 and epsilon = 0.2, the training score over time improved to 5.1094 and 4.90674 respectively. I also tuned alpha and and gamma to optimize the scores for each algorithm. I also made sure to compare the Q-learning and SARSA algorithm with the same hyperparameters set to evaluate their performence for this taxi problem. Overall, large alpha (learning rate) benefits Q-learning a lot but is really terrible for SARSA. For example, for episodesOfTraining set at 5000, setting alpha to a bigger 0.8, gamma = 0.95 and epsilon = 0.2 gives a training score over time of 1.2424 for Q-learning and -9.6384 for SARSA, which is quite a big difference. After all the continuous fine tuning, I have arrived at alpha = 0.1, gamma = 0.9 and epsilon = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future Improvements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very useful to think about the tradeoffs of using different RL algorithms since different RL methods are better for solving different problems. For this taxi problem, I believe that using a deep learning method such as deep Q learning or SARSA (neural networks) can improve the scores but have the significant tradeoffs of memory and time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo and Time step Observation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ----- You can adjust max_steps and epsiodesOfTesting for each algorithm below -----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 500,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Episodes for testing\n",
    "episodesOfTesting = 2 \n",
    "\n",
    "# Maximum steps per episode\n",
    "max_steps = 25\n",
    "\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "# Learning rate\n",
    "alpha = 0.1\n",
    "\n",
    "#Discount factor\n",
    "gamma = 0.9 \n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 0.2                  \n",
    "max_epsilon = 1\n",
    "min_epsilon = 0.01         \n",
    "decay = 0.001     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 501,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[43m \u001b[0m|\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[43m \u001b[0m|\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[43m \u001b[0m|\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[43m \u001b[0m|\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[43m \u001b[0m|\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[43m \u001b[0m|\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[43m \u001b[0m|\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[43m \u001b[0m|\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : |\u001b[43m \u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35m\u001b[43mB\u001b[0m\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| :\u001b[43m \u001b[0m| : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| :\u001b[43m \u001b[0m| : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R:\u001b[43m \u001b[0m| : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| :\u001b[43m \u001b[0m| : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| :\u001b[43m \u001b[0m| : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "|\u001b[43m \u001b[0m: | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[43mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[43mY\u001b[0m| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "|\u001b[43m \u001b[0m: | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[43mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[43mR\u001b[0m: | : :\u001b[34;1mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "|\u001b[43m \u001b[0m: | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "| :\u001b[43m \u001b[0m| : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "|\u001b[43m \u001b[0m: | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|R: | : :\u001b[34;1mG\u001b[0m|\n",
      "|\u001b[43m \u001b[0m: | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[35mB\u001b[0m: |\n",
      "+---------+\n",
      "  (West)\n"
     ]
    }
   ],
   "source": [
    "all_episode_rewards = []\n",
    "\n",
    "for episode in range(episodesOfTesting):\n",
    "    state = env.reset()    \n",
    "    done = False\n",
    "    reward_in_episode = 0   \n",
    "    \n",
    "    if epsilon > 0.01:\n",
    "            epsilon = epsilon - decay\n",
    "            \n",
    "    for step in range(max_steps): \n",
    "        env.render()       \n",
    "        threshold = random.uniform(0, 1)       \n",
    "        if threshold > epsilon:\n",
    "            action = np.argmax(Q[state,:])          \n",
    "        else: \n",
    "            action = env.action_space.sample()  \n",
    "        new_state, reward, done, info = env.step(action) \n",
    "        Q[state, action] = Q[state, action] + alpha * (reward + gamma * np.max(Q[new_state, :]) - Q[state, action]) \n",
    "        state = new_state\n",
    "        reward_in_episode = reward_in_episode + reward  \n",
    "        if done:\n",
    "            break\n",
    "            \n",
    "    all_episode_rewards.append(reward_in_episode)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Episodes for testing\n",
    "episodesOfTesting = 2  \n",
    "\n",
    "# Maximum steps per episode\n",
    "max_steps = 25\n",
    "\n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n))\n",
    "\n",
    "# Learning rate\n",
    "alpha = 0.1\n",
    "\n",
    "#Discount factor\n",
    "gamma = 0.9 \n",
    "\n",
    "# Exploration parameters\n",
    "epsilon = 0.2                 \n",
    "max_epsilon = 1\n",
    "min_epsilon = 0.01         \n",
    "decay = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 503,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y|\u001b[43m \u001b[0m: |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| :\u001b[43m \u001b[0m|B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| |\u001b[43m \u001b[0m: | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[43mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[43mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[43mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|\u001b[43mY\u001b[0m| : |B: |\n",
      "+---------+\n",
      "  (Dropoff)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "|\u001b[43m \u001b[0m| : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "|\u001b[43m \u001b[0m: | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "|\u001b[43m \u001b[0m: | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (West)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "|\u001b[43m \u001b[0m: : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| :\u001b[43m \u001b[0m| : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| :\u001b[43m \u001b[0m: : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | :\u001b[43m \u001b[0m| : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : |\u001b[43m \u001b[0m: : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (North)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : :\u001b[43m \u001b[0m: : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (East)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : :\u001b[43m \u001b[0m: |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (Pickup)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : |\u001b[43m \u001b[0m: |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[43mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[43mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n",
      "+---------+\n",
      "|\u001b[34;1mR\u001b[0m: | : :\u001b[35mG\u001b[0m|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |\u001b[43mB\u001b[0m: |\n",
      "+---------+\n",
      "  (South)\n"
     ]
    }
   ],
   "source": [
    "all_episode_rewards = []\n",
    "\n",
    "for episode in range(episodesOfTesting):\n",
    "    state = env.reset()    \n",
    "    done = False\n",
    "    reward_in_episode = 0\n",
    "    \n",
    "    if epsilon > 0.01:\n",
    "        epsilon = epsilon - decay\n",
    "     \n",
    "    if random.uniform(0, 1) > epsilon:\n",
    "        action = np.argmax(Q[state,:]) \n",
    "    else: \n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "    for step in range(max_steps):\n",
    "        env.render()\n",
    "        new_state, reward, done, info = env.step(action)\n",
    "        if random.uniform(0, 1) > epsilon:\n",
    "            new_action = np.argmax(Q[new_state,:])            \n",
    "        else: \n",
    "            new_action = env.action_space.sample()\n",
    "        Q[state, action] = Q[state, action] + alpha * (reward + gamma * Q[new_state, new_action] - Q[state, action]) \n",
    "        state = new_state\n",
    "        action = new_action\n",
    "        reward_in_episode = reward_in_episode + reward\n",
    "        if done:\n",
    "            break      \n",
    "            \n",
    "    all_episode_rewards.append(reward_in_episode)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gym.openai.com/\n",
    "\n",
    "https://www.cse.unsw.edu.au/~cs9417ml/RL1/algorithms.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
